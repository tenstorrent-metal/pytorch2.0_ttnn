[comment]: <> (This README.md was generated by tools/collect_metrics.py.)
[comment]: <> (Please modify docs/README.md.in and/or collect_metrics.py to make permanent changes.)
# `torch_ttnn` Module

The `torch_ttnn` module has a `backend` function, which can used with the `torch.compile()` function.

```python
import torch
import torch_ttnn

# A torch Module
class FooModule(torch.Module):
    ...
# Create a module
module = FooModule()
# Compile the module, with ttnn backend
ttnn_module = torch.compile(module, torch_ttnn.backend)
# Running inference
ttnn_module(input_data)
```

# Results

The table below summarizes the results of running various ML models through our TTNN compiler. For each model, we track whether the run was successful, the number of operations before and after conversion, the number of `to_device` and `from_device` operations, performance metrics, and accuracy.

| Model    | Run Success   |   Torch Ops (Before) |   Torch Ops (Remain) |   To/From_Device Ops |   Original Run Time (s) |   Compiled Run Time(s) |   Accuracy |
|:---------|:--------------|---------------------:|---------------------:|---------------------:|------------------------:|-----------------------:|-----------:|
| ResNet18 | Yes           |                   70 |                   42 |                  132 |                 1.71981 |                9.07353 |   0.999921 |
| BERT     | Yes           |                 1393 |                  539 |                 3862 |                61.4666  |               36.2097  |   0.986412 |

## Explanation of Metrics

**Model**: Name of the model.  
**Run Success**: Indicates whether the model runs successfully after conversion.  
**Torch Ops (Before)**: The number of operations used by the model in the original Torch implementation.  
**Torch Ops (Remain)**: The number of operations used after conversion to TTNN.  
**To/From_Device Ops**: The number of `to/from_device` operations (data transfer to/from the device).  
**Original Run Time (s)**: Execution time (in seconds) of the model before conversion.  
**Compiled Run Time(s)**: Execution time (in seconds) of the model after conversion.  
**Accuracy**: Model accuracy on a predefined test dataset after conversion.  

# Tracer
The tracer dump the information of fx graph such as node's op_name and shape.

For example, you can run this script to parse the information
```
PYTHONPATH=$(pwd) python3 tools/run_torchvision.py --backend torch_stat --backward --profile
ls stat/raw
```

By default, the raw result will be stored at `stat/raw`, and you can run this script to generate the report
```
python3 tools/generate_report.py
ls stat/
```
Now the `stat/` folder have these report
 - `fw_node_count.csv`
 - `bw_node_count.csv`
 - `fw_total_input_size_dist/`
 - `bw_total_input_size_dist/`
 - `fw_total_output_size_dist/`
 - `bw_total_output_size_dist/`
 - `profile/`

The `node_count.csv` show the node with `op_type` appear in the fx graph. This report can help analyze the frequency of op type appear in the graph.

The `*_total_*_size_dist/` statistics the `op_type`'s input/output_size distribution from all fx graph recored in `stat/raw`. This report can help analyze the memory footprint durning the calculation of `op_type`.

 - Notice: the default `input_shapes` in `tools/stat_torchvision.py` is `[1,3,224,224]`, which has dependency with `*_total_*_size_dist/` report.

 - Notice: the [aten ir interface is in there](https://pytorch.org/docs/stable/torch.compiler_ir.html)

[The `profile/` is the tools provided by pytorch](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html), you can open it by the url: chrome://tracing

# Run transformer models
To run transformer model with ttnn backend, run:
```
PYTHONPATH=${TT_METAL_HOME}:$(pwd) python3 tools/run_transformers.py --model "phiyodr/bert-large-finetuned-squad2" --backend torch_ttnn
```

You can also substitute the backend with `torch_stat` to run a reference comparison.

